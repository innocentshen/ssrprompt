{
  "title": "評測中心",
  "newEvaluation": "新建評測",
  "testCases": "測試案例",
  "addTestCase": "新增測試案例",
  "editTestCase": "編輯測試案例",
  "testCaseName": "案例名稱",
  "input": "輸入",
  "expectedOutput": "預期輸出",
  "actualOutput": "實際輸出",
  "criteria": "評測標準",
  "addCriterion": "新增標準",
  "editCriterion": "編輯標準",
  "criterionName": "標準名稱",
  "criterionDescription": "標準描述",
  "weight": "權重",
  "score": "得分",
  "totalScore": "總分",
  "runEvaluation": "執行評測",
  "evaluating": "評測中...",
  "results": "評測結果",
  "passed": "通過",
  "failed": "失敗",
  "pending": "待執行",
  "running": "執行中",
  "completed": "已完成",
  "noTestCases": "暫無測試案例",
  "noResults": "暫無評測結果",
  "selectPrompt": "選擇 Prompt",
  "selectModel": "選擇模型",
  "createFailed": "建立失敗",
  "evaluationCreated": "評測已建立",
  "createEvaluationFailed": "建立評測失敗",
  "addFailed": "新增失敗",
  "updateFailed": "更新失敗",
  "deleteFailed": "刪除失敗",
  "addTestCasesFirst": "請先新增測試案例",
  "selectModelFirst": "請先選擇被測模型",
  "modelOrProviderNotFound": "模型或服務商未找到",
  "evaluationStarted": "評測已啟動，正在背景執行...",
  "createExecutionRecordFailed": "建立執行紀錄失敗",
  "evaluationFailed": "評估失敗",
  "unknownError": "未知錯誤",
  "evaluationComplete": "評測完成",
  "singleTestComplete": "單案例測試完成",
  "evaluationExecutionFailed": "評測執行失敗",
  "evaluationAborted": "評測已被使用者中止",
  "evaluationStopped": "評測已中止",
  "executionRecordDeleted": "執行紀錄已刪除",
  "deleteExecutionRecordFailed": "刪除執行紀錄失敗",
  "evaluationDeleted": "評測已刪除",
  "deleteEvaluationFailed": "刪除評測失敗",
  "copyEvaluationFailed": "複製評測失敗",
  "copyTestCaseFailed": "複製測試案例失敗",
  "copyCriteriaFailed": "複製評價標準失敗",
  "evaluationCopied": "評測已複製",
  "notPassed": "未通過",
  "clickAddFirstTest": "點擊「新增案例」建立第一個測試",
  "passRate": "通過率",
  "inputTokens": "輸入 Token",
  "outputTokens": "輸出 Token",
  "totalTime": "總耗時",
  "scoreOverview": "評分概覽",
  "evaluationSummary": "評測總結",
  "detailedResults": "詳細結果",
  "testCaseNum": "測試案例 #{{num}}",
  "outputComparison": "輸出對比",
  "modelOutput": "模型輸出",
  "noExpectedOutput": "未設定預期輸出",
  "noOutput": "(無輸出)",
  "errorMessage": "錯誤訊息",
  "scoreDetails": "評分詳情",
  "testNotes": "測試備註",
  "attachments": "附件",
  "attachmentsCount": "{{count}} 個附件",
  "saving": "儲存中...",
  "draftChanges": "未提交",
  "draftChangesHint": "修改不會自動儲存，需要提交新版後才會儲存。",
  "submitNewVersion": "提交新版",
  "submitNewVersionToRun": "目前有未提交修改，請先提交新版後再執行。",
  "saved": "已儲存",
  "runThisCase": "執行此案例",
  "enterTestCaseName": "為測試案例命名",
  "inputText": "輸入文字",
  "expandEdit": "放大編輯",
  "preview": "預覽",
  "noContent": "無內容",
  "enterTestContent": "輸入要測試的文字內容...",
  "variableValues": "變數值",
  "valueOfVar": "{{name}} 的值",
  "clickToPreview": "點擊預覽",
  "deleteAttachment": "刪除附件",
  "addAttachment": "新增附件",
  "expectedOutputOptional": "預期輸出 (選填)",
  "expectedOutputPlaceholder": "預期的模型輸出，用於對比評估...",
  "notesOptional": "備註 (選填)",
  "notesPlaceholder": "新增備註資訊，如測試目的、注意事項等（不會傳送給 AI）...",
  "notesHint": "備註內容僅供參考，不會在評測時傳送給 AI",
  "autoSaveHint": "輸入內容在失焦時自動儲存",
  "editInputText": "編輯輸入文字",
  "editExpectedOutput": "編輯預期輸出",
  "weight": "權重",
  "criterionName": "標準名稱",
  "criterionDescription": "描述",
  "criterionDescPlaceholder": "簡要描述這個評價標準",
  "evaluationPrompt": "評價提示詞",
  "promptVariablesHint": "可用變數: {{input}} (輸入), {{output}} (輸出), {{expected}} (預期輸出)",
  "aiCriteria": "AI 評價標準",
  "addCriterion": "新增標準",
  "selectTemplateOrCustom": "選擇預設範本或建立自訂標準",
  "customCriterion": "+ 自訂標準",
  "createOwnPrompt": "建立自己的評價提示詞",
  "noCriteriaConfigured": "尚未設定評價標準，點擊上方按鈕新增",
  "weightExplanation": "權重說明：",
  "weightTip1": "權重用於計算加權平均分，權重越高影響越大",
  "weightTip2": "建議將重要標準設為 1.5-2.0，次要標準設為 0.5-1.0",
  "weightTip3": "可以透過右側開關暫時停用某個標準，被停用的標準不參與評測",
  "accuracy": "準確性",
  "accuracyDesc": "評估輸出內容的準確性和正確性",
  "relevance": "相關性",
  "relevanceDesc": "評估輸出是否切題和相關",
  "clarity": "清晰度",
  "clarityDesc": "評估輸出的清晰度和可讀性",
  "completeness": "完整性",
  "completenessDesc": "評估輸出的完整性",
  "customCriterionName": "自訂標準",
  "executionHistory": "執行歷史",
  "totalExecutions": "共 {{count}} 次執行",
  "executionNum": "第 {{num}} 次執行",
  "noExecutionRecords": "暫無執行紀錄",
  "clickRunToStart": "點擊「執行評測」開始第一次執行",
  "executing": "執行中...",
  "abort": "中止",
  "tokenConsumption": "Token 消耗",
  "duration": "耗時",
  "noEvaluations": "暫無評測任務",
  "createdAt": "建立於",
  "linkedPrompt": "關聯 Prompt",
  "noLinkedPrompt": "不關聯 Prompt",
  "currentVersion": "目前使用版本",
  "targetModel": "被測模型",
  "judgeModel": "評價模型 (Judge)",
  "noJudgeModel": "不使用AI評價",
  "passThreshold": "通過閾值",
  "threshold10": "10分 (滿分通過)",
  "threshold9": "9分以上",
  "threshold8": "8分以上",
  "threshold7": "7分以上",
  "threshold6": "6分以上 (預設)",
  "threshold5": "5分以上",
  "threshold4": "4分以上",
  "threshold3": "3分以上",
  "threshold0": "不限制",
  "fileProcessing": "附件處理",
  "fileProcessingAuto": "自動（視覺直傳 / 非視覺 OCR）",
  "fileProcessingVision": "直傳（模型直接讀取附件）",
  "fileProcessingOcr": "OCR（將圖片/PDF 轉文字）",
  "fileProcessingNone": "不處理（評測時忽略附件）",
  "ocrProviderFollow": "OCR 服務商：跟隨設定",
  "ocrProviderDatalab": "Datalab（實驗）",
  "status": "狀態",
  "testCasesCount": "測試用例 ({{count}})",
  "criteriaCount": "評價標準 ({{count}})",
  "executionHistoryCount": "執行歷史 ({{count}})",
  "resultsCount": "評測結果 ({{count}})",
  "loadingDetails": "載入評測詳情中...",
  "currentViewing": "目前查看:",
  "viewOtherRecords": "查看其他執行紀錄",
  "noResultsYet": "暫無評測結果",
  "addTestCasesAndRun": "新增測試用例後點擊「執行評測」",
  "selectEvaluationToView": "選擇一個評測任務查看詳情",
  "evaluationName": "評測名稱",
  "evaluationNamePlaceholder": "為評測取個名字",
  "linkedPromptOptional": "關聯 Prompt (選填)",
  "nameCannotBeEmpty": "名稱不能為空",
  "copy": "副本",
  "summaryTemplate": "共 {{total}} 個測試用例，通過 {{passed}} 個，通過率 {{rate}}%",
  "judgePromptTemplate": "你是一個評測專家，請根據以下評價標準對模型輸出進行評分。\n\n評價標準：{{criterionName}}\n{{criterionDescription}}\n{{criterionPrompt}}\n\n使用者輸入：\n{{userInput}}\n\n模型輸出：\n{{modelOutput}}\n\n{{expectedOutput}}\n\n請給出 0-10 的評分和簡短的評價理由。\n回覆格式：\n分數：[0-10的數字]\n理由：[簡短評價]",
  "judgeDescriptionPrefix": "描述：",
  "judgePromptPrefix": "評判提示：",
  "judgeExpectedOutputPrefix": "期望輸出（供參考）：\n",
  "judgeScorePattern": "分數",
  "judgeReasonPattern": "理由",
  "criteriaPromptAccuracy": "請評估以下AI輸出的準確性。\n\n輸入: {{input}}\n{{#expected}}期望輸出: {{expected}}{{/expected}}\n實際輸出: {{output}}\n\n請從以下角度評估：\n1. 資訊是否準確無誤\n2. 是否存在事實性錯誤\n3. 邏輯是否正確\n\n請給出1-10分的評分，並簡要說明理由。\n格式: {\"score\": 數字, \"reason\": \"理由\"}",
  "criteriaPromptRelevance": "請評估以下AI輸出的相關性。\n\n輸入: {{input}}\n實際輸出: {{output}}\n\n請從以下角度評估：\n1. 輸出是否回答了問題\n2. 內容是否與主題相關\n3. 是否存在離題內容\n\n請給出1-10分的評分，並簡要說明理由。\n格式: {\"score\": 數字, \"reason\": \"理由\"}",
  "criteriaPromptClarity": "請評估以下AI輸出的清晰度。\n\n實際輸出: {{output}}\n\n請從以下角度評估：\n1. 表達是否清晰易懂\n2. 結構是否合理\n3. 語言是否流暢\n\n請給出1-10分的評分，並簡要說明理由。\n格式: {\"score\": 數字, \"reason\": \"理由\"}",
  "criteriaPromptCompleteness": "請評估以下AI輸出的完整性。\n\n輸入: {{input}}\n{{#expected}}期望輸出: {{expected}}{{/expected}}\n實際輸出: {{output}}\n\n請從以下角度評估：\n1. 回答是否完整\n2. 是否遺漏重要資訊\n3. 深度是否足夠\n\n請給出1-10分的評分，並簡要說明理由。\n格式: {\"score\": 數字, \"reason\": \"理由\"}",
  "criteriaPromptCustom": "請評估以下AI輸出。\n\n輸入: {{input}}\n{{#expected}}期望輸出: {{expected}}{{/expected}}\n實際輸出: {{output}}\n\n請給出1-10分的評分，並簡要說明理由。\n格式: {\"score\": 數字, \"reason\": \"理由\"}",
  "inheritedFromPrompt": "參數已從關聯 Prompt 繼承",
  "modelParameters": "模型參數",
  "reproducibleModel": "可復現模型",
  "reproducibleJudgeModel": "可復現評判模型"
}
