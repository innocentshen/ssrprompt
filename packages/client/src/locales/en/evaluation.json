{
  "title": "Evaluation",
  "newEvaluation": "New Evaluation",
  "testCases": "Test Cases",
  "addTestCase": "Add Test Case",
  "editTestCase": "Edit Test Case",
  "testCaseName": "Test Case Name",
  "input": "Input",
  "expectedOutput": "Expected Output",
  "actualOutput": "Actual Output",
  "criteria": "Criteria",
  "addCriterion": "Add Criterion",
  "editCriterion": "Edit Criterion",
  "criterionName": "Criterion Name",
  "criterionDescription": "Criterion Description",
  "weight": "Weight",
  "score": "Score",
  "totalScore": "Total Score",
  "runEvaluation": "Run Evaluation",
  "evaluating": "Evaluating...",
  "results": "Results",
  "passed": "Passed",
  "failed": "Failed",
  "pending": "Pending",
  "running": "Running",
  "completed": "Completed",
  "noTestCases": "No Test Cases",
  "noResults": "No Results",
  "selectPrompt": "Select Prompt",
  "selectModel": "Select Model",
  "public": "Public",
  "private": "Private",
  "evaluationPublic": "Evaluation is now public",
  "evaluationPrivate": "Evaluation is now private",
  "clickToPublic": "Click to make public",
  "clickToPrivate": "Click to make private",
  "promptMustBePublicFirst": "The linked Prompt must be public first",
  "createFailed": "Creation failed",
  "evaluationCreated": "Evaluation created",
  "createEvaluationFailed": "Failed to create evaluation",
  "addFailed": "Add failed",
  "updateFailed": "Update failed",
  "deleteFailed": "Delete failed",
  "addTestCasesFirst": "Please add test cases first",
  "selectModelFirst": "Please select a model first",
  "modelOrProviderNotFound": "Model or provider not found",
  "evaluationStarted": "Evaluation started, running in background...",
  "createExecutionRecordFailed": "Failed to create execution record",
  "evaluationFailed": "Evaluation failed",
  "unknownError": "Unknown error",
  "evaluationComplete": "Evaluation complete",
  "singleTestComplete": "Single test complete",
  "evaluationExecutionFailed": "Evaluation execution failed",
  "evaluationAborted": "Evaluation aborted by user",
  "evaluationStopped": "Evaluation stopped",
  "executionRecordDeleted": "Execution record deleted",
  "deleteExecutionRecordFailed": "Failed to delete execution record",
  "evaluationDeleted": "Evaluation deleted",
  "deleteEvaluationFailed": "Failed to delete evaluation",
  "copyEvaluationFailed": "Failed to copy evaluation",
  "copyTestCaseFailed": "Failed to copy test case",
  "copyCriteriaFailed": "Failed to copy criteria",
  "evaluationCopied": "Evaluation copied",
  "notPassed": "Not passed",
  "clickAddFirstTest": "Click \"Add Test Case\" to create your first test",
  "passRate": "Pass Rate",
  "inputTokens": "Input Tokens",
  "outputTokens": "Output Tokens",
  "totalTime": "Total Time",
  "scoreOverview": "Score Overview",
  "evaluationSummary": "Evaluation Summary",
  "detailedResults": "Detailed Results",
  "testCaseNum": "Test Case #{{num}}",
  "outputComparison": "Output Comparison",
  "modelOutput": "Model Output",
  "noExpectedOutput": "No expected output set",
  "noOutput": "(No output)",
  "errorMessage": "Error Message",
  "scoreDetails": "Score Details",
  "testNotes": "Test Notes",
  "attachments": "Attachments",
  "attachmentsCount": "{{count}} attachment(s)",
  "saving": "Saving...",
  "saved": "Saved",
  "draftChanges": "Unsubmitted",
  "draftChangesHint": "Edits aren't auto-saved. Submit a new version to persist them.",
  "submitNewVersion": "Submit New Version",
  "submitNewVersionToRun": "You have unsubmitted changes. Submit a new version before running.",
  "runThisCase": "Run this test case",
  "enterTestCaseName": "Name your test case",
  "inputText": "Input Text",
  "expandEdit": "Expand to edit",
  "preview": "Preview",
  "noContent": "No content",
  "enterTestContent": "Enter the text content to test...",
  "variableValues": "Variable Values",
  "valueOfVar": "Value of {{name}}",
  "clickToPreview": "Click to preview",
  "deleteAttachment": "Delete attachment",
  "addAttachment": "Add Attachment",
  "expectedOutputOptional": "Expected Output (Optional)",
  "expectedOutputPlaceholder": "Expected model output for comparison...",
  "notesOptional": "Notes (Optional)",
  "notesPlaceholder": "Add notes such as test purpose, considerations... (not sent to AI)",
  "notesHint": "Notes are for reference only and will not be sent to AI during evaluation",
  "autoSaveHint": "Content is auto-saved when you leave the field",
  "editInputText": "Edit Input Text",
  "editExpectedOutput": "Edit Expected Output",
  "weight": "Weight",
  "criterionName": "Criterion Name",
  "criterionDescription": "Description",
  "criterionDescPlaceholder": "Briefly describe this evaluation criterion",
  "evaluationPrompt": "Evaluation Prompt",
  "promptVariablesHint": "Available variables: {{input}} (input), {{output}} (output), {{expected}} (expected output)",
  "aiCriteria": "AI Evaluation Criteria",
  "addCriterion": "Add Criterion",
  "selectTemplateOrCustom": "Select a preset template or create a custom criterion",
  "customCriterion": "+ Custom Criterion",
  "createOwnPrompt": "Create your own evaluation prompt",
  "noCriteriaConfigured": "No criteria configured, click the button above to add",
  "weightExplanation": "Weight explanation:",
  "weightTip1": "Weight is used for weighted average calculation, higher weight means more impact",
  "weightTip2": "Recommend setting important criteria to 1.5-2.0, secondary criteria to 0.5-1.0",
  "weightTip3": "You can temporarily disable a criterion with the toggle, disabled criteria won't participate in evaluation",
  "accuracy": "Accuracy",
  "accuracyDesc": "Evaluate the accuracy and correctness of the output",
  "relevance": "Relevance",
  "relevanceDesc": "Evaluate whether the output is on-topic and relevant",
  "clarity": "Clarity",
  "clarityDesc": "Evaluate the clarity and readability of the output",
  "completeness": "Completeness",
  "completenessDesc": "Evaluate the completeness of the output",
  "customCriterionName": "Custom Criterion",
  "executionHistory": "Execution History",
  "totalExecutions": "{{count}} executions total",
  "executionNum": "Execution #{{num}}",
  "noExecutionRecords": "No execution records",
  "clickRunToStart": "Click \"Run Evaluation\" to start first execution",
  "executing": "Executing...",
  "abort": "Abort",
  "tokenConsumption": "Token Consumption",
  "duration": "Duration",
  "noEvaluations": "No evaluations",
  "createdAt": "Created at",
  "linkedPrompt": "Linked Prompt",
  "noLinkedPrompt": "No linked Prompt",
  "currentVersion": "Current version",
  "targetModel": "Target Model",
  "judgeModel": "Judge Model",
  "noJudgeModel": "No AI judge",
  "passThreshold": "Pass Threshold",
  "threshold10": "10 (Perfect score)",
  "threshold9": "9 or above",
  "threshold8": "8 or above",
  "threshold7": "7 or above",
  "threshold6": "6 or above (Default)",
  "threshold5": "5 or above",
  "threshold4": "4 or above",
  "threshold3": "3 or above",
  "threshold0": "No limit",
  "fileProcessing": "File Handling",
  "fileProcessingAuto": "Auto (vision direct / non-vision OCR)",
  "fileProcessingVision": "Direct (send files to model)",
  "fileProcessingOcr": "OCR (extract text from images/PDF)",
  "fileProcessingNone": "Ignore (do not send files)",
  "ocrProviderFollow": "OCR provider: follow Settings",
  "ocrProviderPaddleVl": "PaddleOCR-VL",
  "ocrProviderDatalab": "Datalab (experimental)",
  "status": "Status",
  "testCasesCount": "Test Cases ({{count}})",
  "criteriaCount": "Criteria ({{count}})",
  "executionHistoryCount": "Execution History ({{count}})",
  "resultsCount": "Results ({{count}})",
  "loadingDetails": "Loading evaluation details...",
  "currentViewing": "Currently viewing:",
  "viewOtherRecords": "View other execution records",
  "noResultsYet": "No results yet",
  "addTestCasesAndRun": "Add test cases and click \"Run Evaluation\"",
  "selectEvaluationToView": "Select an evaluation to view details",
  "evaluationName": "Evaluation Name",
  "evaluationNamePlaceholder": "Give your evaluation a name",
  "linkedPromptOptional": "Linked Prompt (Optional)",
  "nameCannotBeEmpty": "Name cannot be empty",
  "copy": "Copy",
  "summaryTemplate": "Total {{total}} test cases, {{passed}} passed, pass rate {{rate}}%",
  "judgePromptTemplate": "You are an evaluation expert. Please score the model output based on the following evaluation criteria.\n\nEvaluation Criteria: {{criterionName}}\n{{criterionDescription}}\n{{criterionPrompt}}\n\nUser Input:\n{{userInput}}\n\nModel Output:\n{{modelOutput}}\n\n{{expectedOutput}}\n\nPlease give a score from 0-10 and a brief reason.\nResponse format:\nScore: [number from 0-10]\nReason: [brief evaluation]",
  "judgeDescriptionPrefix": "Description: ",
  "judgePromptPrefix": "Evaluation Prompt: ",
  "judgeExpectedOutputPrefix": "Expected Output (for reference):\n",
  "judgeScorePattern": "Score",
  "judgeReasonPattern": "Reason",
  "criteriaPromptAccuracy": "Please evaluate the accuracy of the following AI output.\n\nInput: {{input}}\n{{#expected}}Expected Output: {{expected}}{{/expected}}\nActual Output: {{output}}\n\nPlease evaluate from the following perspectives:\n1. Is the information accurate\n2. Are there any factual errors\n3. Is the logic correct\n\nPlease give a score from 1-10 and briefly explain the reason.\nFormat: {\"score\": number, \"reason\": \"reason\"}",
  "criteriaPromptRelevance": "Please evaluate the relevance of the following AI output.\n\nInput: {{input}}\nActual Output: {{output}}\n\nPlease evaluate from the following perspectives:\n1. Does the output answer the question\n2. Is the content relevant to the topic\n3. Is there any off-topic content\n\nPlease give a score from 1-10 and briefly explain the reason.\nFormat: {\"score\": number, \"reason\": \"reason\"}",
  "criteriaPromptClarity": "Please evaluate the clarity of the following AI output.\n\nActual Output: {{output}}\n\nPlease evaluate from the following perspectives:\n1. Is the expression clear and easy to understand\n2. Is the structure reasonable\n3. Is the language fluent\n\nPlease give a score from 1-10 and briefly explain the reason.\nFormat: {\"score\": number, \"reason\": \"reason\"}",
  "criteriaPromptCompleteness": "Please evaluate the completeness of the following AI output.\n\nInput: {{input}}\n{{#expected}}Expected Output: {{expected}}{{/expected}}\nActual Output: {{output}}\n\nPlease evaluate from the following perspectives:\n1. Is the answer complete\n2. Is any important information missing\n3. Is the depth sufficient\n\nPlease give a score from 1-10 and briefly explain the reason.\nFormat: {\"score\": number, \"reason\": \"reason\"}",
  "criteriaPromptCustom": "Please evaluate the following AI output.\n\nInput: {{input}}\n{{#expected}}Expected Output: {{expected}}{{/expected}}\nActual Output: {{output}}\n\nPlease give a score from 1-10 and briefly explain the reason.\nFormat: {\"score\": number, \"reason\": \"reason\"}",
  "inheritedFromPrompt": "Parameters inherited from linked Prompt",
  "modelParameters": "Model Parameters",
  "reproducibleModel": "Reproducible model",
  "reproducibleJudgeModel": "Reproducible judge model"
}
