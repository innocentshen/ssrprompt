{
  "title": "評価センター",
  "newEvaluation": "新規評価",
  "testCases": "テストケース",
  "addTestCase": "テストケースを追加",
  "editTestCase": "テストケースを編集",
  "testCaseName": "テストケース名",
  "input": "入力",
  "expectedOutput": "期待される出力",
  "actualOutput": "実際の出力",
  "criteria": "評価基準",
  "addCriterion": "基準を追加",
  "editCriterion": "基準を編集",
  "criterionName": "基準名",
  "criterionDescription": "基準の説明",
  "weight": "重み",
  "score": "スコア",
  "totalScore": "合計スコア",
  "runEvaluation": "評価を実行",
  "evaluating": "評価中...",
  "results": "評価結果",
  "passed": "合格",
  "failed": "不合格",
  "pending": "待機中",
  "running": "実行中",
  "completed": "完了",
  "noTestCases": "テストケースがありません",
  "noResults": "結果がありません",
  "selectPrompt": "プロンプトを選択",
  "selectModel": "モデルを選択",
  "createFailed": "作成に失敗しました",
  "evaluationCreated": "評価を作成しました",
  "createEvaluationFailed": "評価の作成に失敗しました",
  "addFailed": "追加に失敗しました",
  "updateFailed": "更新に失敗しました",
  "deleteFailed": "削除に失敗しました",
  "addTestCasesFirst": "先にテストケースを追加してください",
  "selectModelFirst": "先にモデルを選択してください",
  "modelOrProviderNotFound": "モデルまたはプロバイダーが見つかりません",
  "evaluationStarted": "評価を開始しました。バックグラウンドで実行中...",
  "createExecutionRecordFailed": "実行記録の作成に失敗しました",
  "evaluationFailed": "評価に失敗しました",
  "unknownError": "不明なエラー",
  "evaluationComplete": "評価完了",
  "singleTestComplete": "単一テスト完了",
  "evaluationExecutionFailed": "評価の実行に失敗しました",
  "evaluationAborted": "ユーザーにより評価が中止されました",
  "evaluationStopped": "評価を中止しました",
  "executionRecordDeleted": "実行記録を削除しました",
  "deleteExecutionRecordFailed": "実行記録の削除に失敗しました",
  "evaluationDeleted": "評価を削除しました",
  "deleteEvaluationFailed": "評価の削除に失敗しました",
  "copyEvaluationFailed": "評価のコピーに失敗しました",
  "copyTestCaseFailed": "テストケースのコピーに失敗しました",
  "copyCriteriaFailed": "基準のコピーに失敗しました",
  "evaluationCopied": "評価をコピーしました",
  "notPassed": "不合格",
  "clickAddFirstTest": "「テストケースを追加」をクリックして最初のテストを作成",
  "passRate": "合格率",
  "inputTokens": "入力トークン",
  "outputTokens": "出力トークン",
  "totalTime": "合計時間",
  "scoreOverview": "スコア概要",
  "evaluationSummary": "評価サマリー",
  "detailedResults": "詳細結果",
  "testCaseNum": "テストケース #{{num}}",
  "outputComparison": "出力比較",
  "modelOutput": "モデル出力",
  "noExpectedOutput": "期待される出力が設定されていません",
  "noOutput": "(出力なし)",
  "errorMessage": "エラーメッセージ",
  "scoreDetails": "スコア詳細",
  "testNotes": "テストノート",
  "attachments": "添付ファイル",
  "attachmentsCount": "{{count}} 件の添付ファイル",
  "saving": "保存中...",
  "draftChanges": "未提出",
  "draftChangesHint": "変更は自動保存されません。新版を提出して保存してください。",
  "submitNewVersion": "新版を提出",
  "submitNewVersionToRun": "未提出の変更があります。実行する前に新版を提出してください。",
  "saved": "保存済み",
  "runThisCase": "このテストケースを実行",
  "enterTestCaseName": "テストケース名を入力",
  "inputText": "入力テキスト",
  "expandEdit": "拡大編集",
  "preview": "プレビュー",
  "noContent": "コンテンツなし",
  "enterTestContent": "テストするテキスト内容を入力...",
  "variableValues": "変数値",
  "valueOfVar": "{{name}} の値",
  "clickToPreview": "クリックしてプレビュー",
  "deleteAttachment": "添付ファイルを削除",
  "addAttachment": "添付ファイルを追加",
  "expectedOutputOptional": "期待される出力 (オプション)",
  "expectedOutputPlaceholder": "比較用の期待されるモデル出力...",
  "notesOptional": "メモ (オプション)",
  "notesPlaceholder": "テスト目的や注意点などのメモを追加... (AIには送信されません)",
  "notesHint": "メモは参考用であり、評価時にAIには送信されません",
  "autoSaveHint": "フィールドから離れると自動保存されます",
  "editInputText": "入力テキストを編集",
  "editExpectedOutput": "期待される出力を編集",
  "weight": "重み",
  "criterionName": "基準名",
  "criterionDescription": "説明",
  "criterionDescPlaceholder": "この評価基準を簡潔に説明してください",
  "evaluationPrompt": "評価プロンプト",
  "promptVariablesHint": "利用可能な変数: {{input}} (入力), {{output}} (出力), {{expected}} (期待される出力)",
  "aiCriteria": "AI評価基準",
  "addCriterion": "基準を追加",
  "selectTemplateOrCustom": "プリセットテンプレートを選択するか、カスタム基準を作成",
  "customCriterion": "+ カスタム基準",
  "createOwnPrompt": "独自の評価プロンプトを作成",
  "noCriteriaConfigured": "評価基準が設定されていません。上のボタンをクリックして追加",
  "weightExplanation": "重みの説明：",
  "weightTip1": "重みは加重平均の計算に使用され、重みが高いほど影響が大きくなります",
  "weightTip2": "重要な基準は1.5-2.0、副次的な基準は0.5-1.0に設定することを推奨",
  "weightTip3": "右側のトグルで基準を一時的に無効にできます。無効な基準は評価に参加しません",
  "accuracy": "正確性",
  "accuracyDesc": "出力の正確性と正しさを評価",
  "relevance": "関連性",
  "relevanceDesc": "出力が的確で関連性があるかを評価",
  "clarity": "明確さ",
  "clarityDesc": "出力の明確さと読みやすさを評価",
  "completeness": "完全性",
  "completenessDesc": "出力の完全性を評価",
  "customCriterionName": "カスタム基準",
  "executionHistory": "実行履歴",
  "totalExecutions": "合計 {{count}} 回実行",
  "executionNum": "第 {{num}} 回実行",
  "noExecutionRecords": "実行記録がありません",
  "clickRunToStart": "「評価を実行」をクリックして最初の実行を開始",
  "executing": "実行中...",
  "abort": "中止",
  "tokenConsumption": "トークン消費",
  "duration": "所要時間",
  "noEvaluations": "評価がありません",
  "createdAt": "作成日時",
  "linkedPrompt": "関連プロンプト",
  "noLinkedPrompt": "プロンプトなし",
  "currentVersion": "現在のバージョン",
  "targetModel": "対象モデル",
  "judgeModel": "評価モデル (Judge)",
  "noJudgeModel": "AI評価なし",
  "passThreshold": "合格閾値",
  "threshold10": "10点 (満点)",
  "threshold9": "9点以上",
  "threshold8": "8点以上",
  "threshold7": "7点以上",
  "threshold6": "6点以上 (デフォルト)",
  "threshold5": "5点以上",
  "threshold4": "4点以上",
  "threshold3": "3点以上",
  "threshold0": "制限なし",
  "fileProcessing": "添付処理",
  "fileProcessingAuto": "自動（ビジョン直送 / 非ビジョン OCR）",
  "fileProcessingVision": "直送（モデルが添付を直接処理）",
  "fileProcessingOcr": "OCR（画像/PDF をテキスト化）",
  "fileProcessingNone": "無視（添付を送信しない）",
  "ocrProviderFollow": "OCR：設定に従う",
  "ocrProviderPaddleVl": "PaddleOCR-VL",
  "ocrProviderDatalab": "Datalab（実験）",
  "status": "ステータス",
  "testCasesCount": "テストケース ({{count}})",
  "criteriaCount": "評価基準 ({{count}})",
  "executionHistoryCount": "実行履歴 ({{count}})",
  "resultsCount": "結果 ({{count}})",
  "loadingDetails": "評価詳細を読み込み中...",
  "currentViewing": "現在表示中:",
  "viewOtherRecords": "他の実行記録を表示",
  "noResultsYet": "結果がありません",
  "addTestCasesAndRun": "テストケースを追加して「評価を実行」をクリック",
  "selectEvaluationToView": "評価を選択して詳細を表示",
  "evaluationName": "評価名",
  "evaluationNamePlaceholder": "評価に名前を付けてください",
  "linkedPromptOptional": "関連プロンプト (オプション)",
  "nameCannotBeEmpty": "名前は必須です",
  "copy": "コピー",
  "summaryTemplate": "合計 {{total}} テストケース、{{passed}} 件合格、合格率 {{rate}}%",
  "judgePromptTemplate": "あなたは評価の専門家です。以下の評価基準に基づいてモデル出力を採点してください。\n\n評価基準：{{criterionName}}\n{{criterionDescription}}\n{{criterionPrompt}}\n\nユーザー入力：\n{{userInput}}\n\nモデル出力：\n{{modelOutput}}\n\n{{expectedOutput}}\n\n0-10のスコアと簡潔な理由を提供してください。\n回答形式：\nスコア：[0-10の数字]\n理由：[簡潔な評価]",
  "judgeDescriptionPrefix": "説明：",
  "judgePromptPrefix": "評価プロンプト：",
  "judgeExpectedOutputPrefix": "期待される出力（参考）：\n",
  "judgeScorePattern": "スコア",
  "judgeReasonPattern": "理由",
  "criteriaPromptAccuracy": "以下のAI出力の正確性を評価してください。\n\n入力: {{input}}\n{{#expected}}期待される出力: {{expected}}{{/expected}}\n実際の出力: {{output}}\n\n以下の観点から評価してください：\n1. 情報は正確か\n2. 事実誤認はないか\n3. 論理は正しいか\n\n1-10のスコアと簡潔な理由を提供してください。\n形式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptRelevance": "以下のAI出力の関連性を評価してください。\n\n入力: {{input}}\n実際の出力: {{output}}\n\n以下の観点から評価してください：\n1. 出力は質問に答えているか\n2. 内容はトピックに関連しているか\n3. 話題から外れた内容はないか\n\n1-10のスコアと簡潔な理由を提供してください。\n形式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptClarity": "以下のAI出力の明確さを評価してください。\n\n実際の出力: {{output}}\n\n以下の観点から評価してください：\n1. 表現は明確で理解しやすいか\n2. 構造は合理的か\n3. 言語は流暢か\n\n1-10のスコアと簡潔な理由を提供してください。\n形式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptCompleteness": "以下のAI出力の完全性を評価してください。\n\n入力: {{input}}\n{{#expected}}期待される出力: {{expected}}{{/expected}}\n実際の出力: {{output}}\n\n以下の観点から評価してください：\n1. 回答は完全か\n2. 重要な情報が欠けていないか\n3. 深さは十分か\n\n1-10のスコアと簡潔な理由を提供してください。\n形式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptCustom": "以下のAI出力を評価してください。\n\n入力: {{input}}\n{{#expected}}期待される出力: {{expected}}{{/expected}}\n実際の出力: {{output}}\n\n1-10のスコアと簡潔な理由を提供してください。\n形式: {\"score\": 数字, \"reason\": \"理由\"}",
  "inheritedFromPrompt": "関連Promptからパラメータを継承",
  "modelParameters": "モデルパラメータ",
  "reproducibleModel": "再現可能なモデル",
  "reproducibleJudgeModel": "再現可能な評価モデル"
}
