{
  "title": "评测中心",
  "newEvaluation": "新建评测",
  "testCases": "测试用例",
  "addTestCase": "添加测试用例",
  "editTestCase": "编辑测试用例",
  "testCaseName": "用例名称",
  "input": "输入",
  "expectedOutput": "期望输出",
  "actualOutput": "实际输出",
  "criteria": "评测标准",
  "addCriterion": "添加标准",
  "editCriterion": "编辑标准",
  "criterionName": "标准名称",
  "criterionDescription": "标准描述",
  "weight": "权重",
  "score": "得分",
  "totalScore": "总分",
  "runEvaluation": "运行评测",
  "evaluating": "评测中...",
  "results": "评测结果",
  "passed": "通过",
  "failed": "失败",
  "pending": "待运行",
  "running": "运行中",
  "completed": "已完成",
  "noTestCases": "暂无测试用例",
  "noResults": "暂无评测结果",
  "selectPrompt": "选择 Prompt",
  "selectModel": "选择模型",
  "public": "公开",
  "private": "私有",
  "evaluationPublic": "评测已设为公开",
  "evaluationPrivate": "评测已设为私有",
  "clickToPublic": "点击设为公开",
  "clickToPrivate": "点击设为私有",
  "promptMustBePublicFirst": "需要先将关联的 Prompt 设为公开",
  "createFailed": "创建失败",
  "evaluationCreated": "评测已创建",
  "createEvaluationFailed": "创建评测失败",
  "addFailed": "添加失败",
  "updateFailed": "更新失败",
  "deleteFailed": "删除失败",
  "addTestCasesFirst": "请先添加测试用例",
  "selectModelFirst": "请先选择被测模型",
  "modelOrProviderNotFound": "模型或服务商未找到",
  "evaluationStarted": "评测已启动，正在后台运行...",
  "createExecutionRecordFailed": "创建执行记录失败",
  "evaluationFailed": "评估失败",
  "unknownError": "未知错误",
  "evaluationComplete": "评测完成",
  "singleTestComplete": "单用例测试完成",
  "evaluationExecutionFailed": "评测执行失败",
  "evaluationAborted": "评测已被用户中止",
  "evaluationStopped": "评测已中止",
  "executionRecordDeleted": "执行记录已删除",
  "deleteExecutionRecordFailed": "删除执行记录失败",
  "evaluationDeleted": "评测已删除",
  "deleteEvaluationFailed": "删除评测失败",
  "copyEvaluationFailed": "复制评测失败",
  "copyTestCaseFailed": "复制测试用例失败",
  "copyCriteriaFailed": "复制评价标准失败",
  "evaluationCopied": "评测已复制",
  "notPassed": "未通过",
  "clickAddFirstTest": "点击\"添加用例\"创建第一个测试",
  "passRate": "通过率",
  "inputTokens": "输入 Token",
  "outputTokens": "输出 Token",
  "totalTime": "总耗时",
  "scoreOverview": "评分概览",
  "evaluationSummary": "评测总结",
  "detailedResults": "详细结果",
  "testCaseNum": "测试用例 #{{num}}",
  "outputComparison": "输出对比",
  "modelOutput": "模型输出",
  "noExpectedOutput": "未设置期望输出",
  "noOutput": "(无输出)",
  "errorMessage": "错误信息",
  "scoreDetails": "评分详情",
  "testNotes": "测试备注",
  "attachments": "附件",
  "attachmentsCount": "{{count}} 个附件",
  "saving": "保存中...",
  "saved": "已保存",
  "draftChanges": "未提交",
  "draftChangesHint": "修改不会自动保存，需要提交新版后才会保存。",
  "submitNewVersion": "提交新版",
  "submitNewVersionToRun": "当前有未提交修改，请先提交新版后再运行。",
  "runThisCase": "运行此用例",
  "enterTestCaseName": "给测试用例起个名字",
  "inputText": "输入文本",
  "expandEdit": "放大编辑",
  "preview": "预览",
  "noContent": "无内容",
  "enterTestContent": "输入要测试的文本内容...",
  "variableValues": "变量值",
  "valueOfVar": "{{name}} 的值",
  "clickToPreview": "点击预览",
  "deleteAttachment": "删除附件",
  "addAttachment": "添加附件",
  "expectedOutputOptional": "期望输出 (可选)",
  "expectedOutputPlaceholder": "期望的模型输出，用于对比评估...",
  "notesOptional": "备注 (可选)",
  "notesPlaceholder": "添加备注信息，如测试目的、注意事项等（不会发送给 AI）...",
  "notesHint": "备注内容仅供参考，不会在评测时发送给 AI",
  "autoSaveHint": "输入内容在失焦时自动保存",
  "editInputText": "编辑输入文本",
  "editExpectedOutput": "编辑期望输出",
  "criterionDescPlaceholder": "简要描述这个评价标准",
  "evaluationPrompt": "评价提示词",
  "promptVariablesHint": "可用变量: {{input}} (输入), {{output}} (输出), {{expected}} (期望输出)",
  "aiCriteria": "AI 评价标准",
  "selectTemplateOrCustom": "选择预设模板或创建自定义标准",
  "customCriterion": "+ 自定义标准",
  "createOwnPrompt": "创建自己的评价提示词",
  "noCriteriaConfigured": "暂未配置评价标准，点击上方按钮添加",
  "weightExplanation": "权重说明：",
  "weightTip1": "权重用于计算加权平均分，权重越高影响越大",
  "weightTip2": "建议将重要标准设置为 1.5-2.0，次要标准设置为 0.5-1.0",
  "weightTip3": "可以通过右侧开关临时禁用某个标准，被禁用的标准不参与评测",
  "accuracy": "准确性",
  "accuracyDesc": "评估输出内容的准确性和正确性",
  "relevance": "相关性",
  "relevanceDesc": "评估输出是否切题和相关",
  "clarity": "清晰度",
  "clarityDesc": "评估输出的清晰度和可读性",
  "completeness": "完整性",
  "completenessDesc": "评估输出的完整性",
  "customCriterionName": "自定义标准",
  "executionHistory": "执行历史",
  "totalExecutions": "共 {{count}} 次执行",
  "executionNum": "第 {{num}} 次执行",
  "noExecutionRecords": "暂无执行记录",
  "clickRunToStart": "点击\"运行评测\"开始第一次执行",
  "executing": "执行中...",
  "abort": "中止",
  "tokenConsumption": "Token 消耗",
  "duration": "耗时",
  "noEvaluations": "暂无评测任务",
  "createdAt": "创建于",
  "linkedPrompt": "关联 Prompt",
  "noLinkedPrompt": "不关联 Prompt",
  "currentVersion": "当前使用版本",
  "targetModel": "被测模型",
  "judgeModel": "评价模型 (Judge)",
  "noJudgeModel": "不使用AI评价",
  "passThreshold": "通过阈值",
  "threshold10": "10分 (满分通过)",
  "threshold9": "9分以上",
  "threshold8": "8分以上",
  "threshold7": "7分以上",
  "threshold6": "6分以上 (默认)",
  "threshold5": "5分以上",
  "threshold4": "4分以上",
  "threshold3": "3分以上",
  "threshold0": "不限制",
  "fileProcessing": "附件处理",
  "fileProcessingAuto": "自动（视觉直传 / 非视觉 OCR）",
  "fileProcessingVision": "直传（模型直接读取附件）",
  "fileProcessingOcr": "OCR（将图片/PDF 转文字）",
  "fileProcessingNone": "不处理（评测时忽略附件）",
  "ocrProviderFollow": "OCR 服务商：跟随设置",
  "ocrProviderPaddleVl": "PaddleOCR-VL",
  "ocrProviderDatalab": "Datalab（实验）",
  "status": "状态",
  "testCasesCount": "测试用例 ({{count}})",
  "criteriaCount": "评价标准 ({{count}})",
  "executionHistoryCount": "执行历史 ({{count}})",
  "resultsCount": "评测结果 ({{count}})",
  "loadingDetails": "加载评测详情中...",
  "currentViewing": "当前查看:",
  "viewOtherRecords": "查看其他执行记录",
  "noResultsYet": "暂无评测结果",
  "addTestCasesAndRun": "添加测试用例后点击\"运行评测\"",
  "selectEvaluationToView": "选择一个评测任务查看详情",
  "evaluationName": "评测名称",
  "evaluationNamePlaceholder": "给评测起个名字",
  "linkedPromptOptional": "关联 Prompt (可选)",
  "nameCannotBeEmpty": "名称不能为空",
  "copy": "副本",
  "summaryTemplate": "共 {{total}} 个测试用例，通过 {{passed}} 个，通过率 {{rate}}%",
  "judgePromptTemplate": "你是一个评测专家，请根据以下评价标准对模型输出进行评分。\n\n评价标准：{{criterionName}}\n{{criterionDescription}}\n{{criterionPrompt}}\n\n用户输入：\n{{userInput}}\n\n模型输出：\n{{modelOutput}}\n\n{{expectedOutput}}\n\n请给出 0-10 的评分和简短的评价理由。\n回复格式：\n分数：[0-10的数字]\n理由：[简短评价]",
  "judgeDescriptionPrefix": "描述：",
  "judgePromptPrefix": "评判提示：",
  "judgeExpectedOutputPrefix": "期望输出（供参考）：\n",
  "judgeScorePattern": "分数",
  "judgeReasonPattern": "理由",
  "criteriaPromptAccuracy": "请评估以下AI输出的准确性。\n\n输入: {{input}}\n{{#expected}}期望输出: {{expected}}{{/expected}}\n实际输出: {{output}}\n\n请从以下角度评估：\n1. 信息是否准确无误\n2. 是否存在事实性错误\n3. 逻辑是否正确\n\n请给出1-10分的评分，并简要说明理由。\n格式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptRelevance": "请评估以下AI输出的相关性。\n\n输入: {{input}}\n实际输出: {{output}}\n\n请从以下角度评估：\n1. 输出是否回答了问题\n2. 内容是否与主题相关\n3. 是否存在离题内容\n\n请给出1-10分的评分，并简要说明理由。\n格式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptClarity": "请评估以下AI输出的清晰度。\n\n实际输出: {{output}}\n\n请从以下角度评估：\n1. 表达是否清晰易懂\n2. 结构是否合理\n3. 语言是否流畅\n\n请给出1-10分的评分，并简要说明理由。\n格式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptCompleteness": "请评估以下AI输出的完整性。\n\n输入: {{input}}\n{{#expected}}期望输出: {{expected}}{{/expected}}\n实际输出: {{output}}\n\n请从以下角度评估：\n1. 回答是否完整\n2. 是否遗漏重要信息\n3. 深度是否足够\n\n请给出1-10分的评分，并简要说明理由。\n格式: {\"score\": 数字, \"reason\": \"理由\"}",
  "criteriaPromptCustom": "请评估以下AI输出。\n\n输入: {{input}}\n{{#expected}}期望输出: {{expected}}{{/expected}}\n实际输出: {{output}}\n\n请给出1-10分的评分，并简要说明理由。\n格式: {\"score\": 数字, \"reason\": \"理由\"}",
  "inheritedFromPrompt": "参数已从关联 Prompt 继承",
  "modelParameters": "模型参数",
  "reproducibleModel": "可复现模型",
  "reproducibleJudgeModel": "可复现评判模型"
}
